diff --git a/include/linux/myref.h b/include/linux/myref.h
new file mode 100644
index 000000000..b58467d64
--- /dev/null
+++ b/include/linux/myref.h
@@ -0,0 +1,12 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _LINUX_MYREF_H
+#define _LINUX_MYREF_H
+
+#include <linux/fs.h>
+#include <linux/mm_types.h>
+
+int my_folio_get(struct folio *folio);
+int my_folio_put(struct folio *folio);
+bool my_folio_can_freeze(struct folio *folio, int count);
+
+#endif /* _LINUX_MYREF_H */
diff --git a/include/linux/paygo.h b/include/linux/paygo.h
new file mode 100644
index 000000000..3f1491b4b
--- /dev/null
+++ b/include/linux/paygo.h
@@ -0,0 +1,48 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _LINUX_PAYGO_H
+#define _LINUX_PAYGO_H
+
+
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <asm/atomic.h>
+
+
+#define TABLESIZE (4)
+#define HASHSHIFT (11)
+
+struct paygo_entry {
+	void *obj;
+	int local_counter;
+	atomic_t anchor_counter;
+	struct list_head list;
+
+
+	// x,y are used for debug
+	// x means local inc to the obj
+	// y means local dec to the obj
+
+	// (x + y) = entry->local_counter (always)
+	// (x + y) + anchor_counter = 0 when the program end (i.e. After all inc and dec pairs have been terminated)
+	int x;
+	int y;
+} ____cacheline_aligned_in_smp;
+
+struct overflow {
+	spinlock_t lock;
+	struct list_head head;
+};
+
+struct paygo {
+	struct paygo_entry entries[TABLESIZE];
+	struct overflow overflow_lists[TABLESIZE];
+};
+
+void init_paygo_table(void);
+int paygo_inc(void *obj);
+int paygo_dec(void *obj);
+bool paygo_read(void *obj);
+void traverse_paygo(void);
+
+
+#endif /* _LINUX_PAYGO_H */
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 853d08f75..03b6ada97 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -734,6 +734,12 @@ struct kmap_ctrl {
 #endif
 };
 
+struct anchor_info {
+	int cpu;
+	void *obj;
+	struct list_head list;
+};
+
 struct task_struct {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/*
@@ -749,6 +755,8 @@ struct task_struct {
 	unsigned int			saved_state;
 #endif
 
+	struct list_head anchor_info_list;
+
 	/*
 	 * This begins the randomizable portion of task_struct. Only
 	 * scheduling-critical items should be added above here.
diff --git a/init/init_task.c b/init/init_task.c
index ff6c4b9bf..0eb4ea518 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -71,6 +71,7 @@ struct task_struct init_task
 	.thread_info	= INIT_THREAD_INFO(init_task),
 	.stack_refcount	= REFCOUNT_INIT(1),
 #endif
+	.anchor_info_list = LIST_HEAD_INIT(init_task.anchor_info_list),
 	.__state	= 0,
 	.stack		= init_stack,
 	.usage		= REFCOUNT_INIT(2),
diff --git a/kernel/fork.c b/kernel/fork.c
index 9f7fe3541..8b555b78b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2147,6 +2147,9 @@ static __latent_entropy struct task_struct *copy_process(
 	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
 	p->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE | PF_NO_SETAFFINITY);
 	p->flags |= PF_FORKNOEXEC;
+	
+	INIT_LIST_HEAD(&p->anchor_info_list);
+
 	INIT_LIST_HEAD(&p->children);
 	INIT_LIST_HEAD(&p->sibling);
 	rcu_copy_process(p);
diff --git a/mm/Makefile b/mm/Makefile
index 8e105e5b3..132145282 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -66,6 +66,8 @@ memory-hotplug-$(CONFIG_MEMORY_HOTPLUG) += memory_hotplug.o
 obj-y += page-alloc.o
 obj-y += init-mm.o
 obj-y += memblock.o
+obj-y += paygo.o
+obj-y += myref.o
 obj-y += $(memory-hotplug-y)
 
 ifdef CONFIG_MMU
diff --git a/mm/filemap.c b/mm/filemap.c
index 0e20a8d6d..a1b63bb4c 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -10,6 +10,8 @@
  * most "normal" filesystems (but you don't /have/ to use this:
  * the NFS filesystem used to do this differently, for example)
  */
+#include <linux/paygo.h>
+#include <linux/myref.h>
 #include <linux/export.h>
 #include <linux/compiler.h>
 #include <linux/dax.h>
@@ -840,6 +842,7 @@ EXPORT_SYMBOL_GPL(replace_page_cache_folio);
 noinline int __filemap_add_folio(struct address_space *mapping,
 		struct folio *folio, pgoff_t index, gfp_t gfp, void **shadowp)
 {
+	int i;
 	XA_STATE(xas, &mapping->i_pages, index);
 	int huge = folio_test_hugetlb(folio);
 	bool charged = false;
@@ -860,7 +863,10 @@ noinline int __filemap_add_folio(struct address_space *mapping,
 	}
 
 	gfp &= GFP_RECLAIM_MASK;
-	folio_ref_add(folio, nr);
+	//folio_ref_add(folio, nr);
+	for(i = 0; i < nr; i++) {
+		my_folio_get(folio);
+	}
 	folio->mapping = mapping;
 	folio->index = xas.xa_index;
 
@@ -920,7 +926,10 @@ noinline int __filemap_add_folio(struct address_space *mapping,
 		mem_cgroup_uncharge(folio);
 	folio->mapping = NULL;
 	/* Leave page->index set: truncation relies upon it */
-	folio_put_refs(folio, nr);
+//	folio_put_refs(folio, nr);
+	for(i = 0; i < nr; i++) {
+		my_folio_put(folio);
+	}
 	return xas_error(&xas);
 }
 ALLOW_ERROR_INJECTION(__filemap_add_folio, ERRNO);
@@ -1039,6 +1048,7 @@ void __init pagecache_init(void)
 		init_waitqueue_head(&folio_wait_table[i]);
 
 	page_writeback_init();
+	init_paygo_table();
 }
 
 /*
@@ -2015,11 +2025,14 @@ static inline struct folio *find_get_entry(struct xa_state *xas, pgoff_t max,
 	if (!folio || xa_is_value(folio))
 		return folio;
 
-	if (!folio_try_get_rcu(folio))
+//	if (!folio_try_get_rcu(folio))
+//		goto reset;
+	if (!my_folio_get(folio))
 		goto reset;
 
 	if (unlikely(folio != xas_reload(xas))) {
-		folio_put(folio);
+//		folio_put(folio);
+		my_folio_put(folio);
 		goto reset;
 	}
 
@@ -2123,7 +2136,8 @@ unsigned find_lock_entries(struct address_space *mapping, pgoff_t *start,
 unlock:
 		folio_unlock(folio);
 put:
-		folio_put(folio);
+//		folio_put(folio);
+		my_folio_put(folio);
 	}
 	rcu_read_unlock();
 
@@ -2384,9 +2398,10 @@ static void filemap_get_read_batch(struct address_space *mapping,
 			break;
 		if (xa_is_sibling(folio))
 			break;
-		if (!folio_try_get_rcu(folio))
-			goto retry;
-
+//		if (!folio_try_get_rcu(folio))
+//			goto retry;
+		if(!my_folio_get(folio))
+			goto retry;	
 		if (unlikely(folio != xas_reload(&xas)))
 			goto put_folio;
 
@@ -2399,7 +2414,8 @@ static void filemap_get_read_batch(struct address_space *mapping,
 		xas_advance(&xas, folio->index + folio_nr_pages(folio) - 1);
 		continue;
 put_folio:
-		folio_put(folio);
+//		folio_put(folio);
+		my_folio_put(folio);
 retry:
 		xas_reset(&xas);
 	}
@@ -2757,6 +2773,7 @@ ssize_t filemap_read(struct kiocb *iocb, struct iov_iter *iter,
 put_folios:
 		for (i = 0; i < folio_batch_count(&fbatch); i++)
 			folio_put(fbatch.folios[i]);
+			//my_folio_put(fbatch.folios[i]);
 		folio_batch_init(&fbatch);
 	} while (iov_iter_count(iter) && iocb->ki_pos < isize && !error);
 
diff --git a/mm/myref.c b/mm/myref.c
new file mode 100644
index 000000000..3f8c774a3
--- /dev/null
+++ b/mm/myref.c
@@ -0,0 +1,47 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * New page cache protocol
+ *
+ * Copyright (C) 2023 Hanyang, Univ. Kun-wook Park
+ */
+
+#include <linux/paygo.h>
+#include <linux/fs.h>
+#include <linux/mm_types.h>
+#include <linux/page_ref.h>
+
+int my_folio_get(struct folio *folio)
+{
+	int refcount;
+	paygo_inc((void*)&folio->page);	
+	refcount = folio_ref_count(folio);	
+	if(refcount) {
+		return true;
+	}
+	paygo_dec((void*)&folio->page);	
+	return false;
+}
+EXPORT_SYMBOL(my_folio_get);
+
+int my_folio_put(struct folio *folio)
+{
+	paygo_dec((void*)&folio->page);	
+	return 0;
+}
+EXPORT_SYMBOL(my_folio_put);
+
+bool my_folio_can_freeze(struct folio *folio, int count)
+{
+	bool can_freeze;
+	if(!folio_ref_freeze(folio, count)) {
+		// CAS fail: It means that another process already starts delete the folio.
+		return false;
+	}	
+	// ref = 0 in here
+	can_freeze = paygo_read((void*)&folio->page);
+	if(can_freeze) 
+		return true;
+	// can_freeze(false) : There may be the users who use the folio.
+	return false;
+}
+EXPORT_SYMBOL(my_folio_can_freeze);
diff --git a/mm/paygo.c b/mm/paygo.c
new file mode 100644
index 000000000..c8eec9490
--- /dev/null
+++ b/mm/paygo.c
@@ -0,0 +1,436 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Paygo reference counting
+ *
+ * Copyright (C) 2023 Hanyang, Univ. Kun-wook Park
+ */
+
+#include <linux/paygo.h>
+#include <linux/percpu.h>
+#include <linux/list.h>
+#include <linux/hashtable.h>
+#include <linux/spinlock.h>
+#include <linux/percpu.h>
+#include <linux/slab.h>
+#include <asm/atomic.h>
+#include <linux/hash.h>
+#include <linux/sched.h>
+
+static DEFINE_PER_CPU(struct paygo *, paygo_table_ptr);
+
+static unsigned long hash_function(const void *obj)
+{
+	unsigned long ret;
+	unsigned long hash;
+	hash = hash_64((unsigned long)obj, HASHSHIFT);
+	ret = hash % TABLESIZE;
+	return ret;
+}
+
+static int push_hash(void *obj)
+{
+	unsigned long hash;
+	struct paygo_entry *entry;
+	struct overflow *ovfl;
+	struct paygo *p = per_cpu(paygo_table_ptr, smp_processor_id());
+
+	hash = hash_function(obj);
+	entry = &p->entries[hash];
+
+	// when the hashtable's entry is NULL
+	if (entry->obj == NULL) {
+		{
+			entry->obj = obj;
+			entry->local_counter = 1;
+			atomic_set(&entry->anchor_counter, 0);
+
+			entry->x = 1;
+			entry->y = 0;
+		}
+		return 0;
+	}
+
+	// when there is NULL in the entry
+	// We need to insert a new entry into the overflow list
+	else {
+		struct paygo_entry *new_entry;
+		ovfl = &p->overflow_lists[hash];
+		new_entry = kzalloc(sizeof(struct paygo_entry), GFP_ATOMIC);
+		if (!new_entry) {
+			return -ENOMEM;
+		}
+		{
+			new_entry->obj = obj;
+			new_entry->local_counter = 1;
+			atomic_set(&new_entry->anchor_counter, 0);
+
+			new_entry->x = 1;
+			new_entry->y = 0;
+		}
+
+		spin_lock(&ovfl->lock);
+		list_add(&new_entry->list, &ovfl->head);
+		spin_unlock(&ovfl->lock);
+		return 0;
+	}
+}
+
+static struct paygo_entry *find_hash(void *obj)
+{
+	int cpu;
+	unsigned long hash;
+	struct paygo_entry *entry;
+	struct overflow *ovfl;
+	struct list_head *pos, *n;
+	struct paygo *p;
+	cpu = smp_processor_id();
+	p = per_cpu(paygo_table_ptr, cpu);
+
+	hash = hash_function(obj);
+
+	entry = &p->entries[hash];
+
+// redo to delete a new entry when the sum of the new entry's counter taken from the overflow list is 0
+redo:
+	if (likely(entry->obj == obj)) {
+		return entry;
+	} else {
+		ovfl = &p->overflow_lists[hash];
+		spin_lock(&ovfl->lock);
+		///////////////////////////////////////////////////////////////////////////////////////////
+		// delete entry on hash table
+		if (unlikely(entry->local_counter +
+				     atomic_read(&(entry->anchor_counter)) ==
+			     0)) {
+			struct paygo_entry *new_entry;
+			if (!list_empty(&ovfl->head)) {
+				new_entry = list_first_entry(
+					&ovfl->head, struct paygo_entry, list);
+				*entry = *new_entry;
+				list_del(&new_entry->list);
+				kfree(new_entry);
+				spin_unlock(&ovfl->lock);
+				// we need to redo and check
+				// 0. Whether or not a new entry in the hashtable is what we were looking for
+				// 1. Whether or not the overflow list is empty
+				goto redo;
+			}
+
+			else {
+				entry->obj = NULL;
+				entry->local_counter = 0;
+				atomic_set(&(entry->anchor_counter), 0);
+				entry->x = 0;
+				entry->y = 0;
+
+				spin_unlock(&ovfl->lock);
+				return NULL;
+			}
+		}
+		///////////////////////////////////////////////////////////////////////////////////////////
+
+		// the hashtable's entry is not the entry that we are looking for.
+		// so we need to search overflow list
+		list_for_each_safe(pos, n, &ovfl->head) {
+			struct paygo_entry *ovfl_entry =
+				list_entry(pos, struct paygo_entry, list);
+
+			if (ovfl_entry->obj == obj) {
+				spin_unlock(&ovfl->lock);
+				return ovfl_entry;
+			}
+		}
+		spin_unlock(&ovfl->lock);
+	}
+
+	return NULL;
+}
+
+static void record_anchor(int cpu, void *obj)
+{
+	struct anchor_info *info;
+	info = kmalloc(sizeof(struct anchor_info), GFP_KERNEL);
+	if (!info) {
+		pr_err("Failed to allocate memory for anchor_info\n");
+		return;
+	}
+	info->cpu = cpu;
+	info->obj = obj;
+	list_add_tail(&info->list, &current->anchor_info_list);
+}
+
+static int unrecord_anchor(void *obj)
+{
+	struct anchor_info *info;
+	int cpu = -1;
+
+	list_for_each_entry_reverse(
+		info, &current->anchor_info_list, list) {
+		if (info->obj == obj) {
+			cpu = info->cpu;
+			list_del(&info->list);
+			kfree(info);
+			break;
+		}
+	}
+
+	// Since all of the unref operations are always followed by ref operations,
+	// there is no situation where anchor information list is empty.
+	if (cpu == -1) {
+		pr_err("Failed to find anchor_info with given obj\n");
+	}
+
+	return cpu;
+}
+
+static void dec_other_entry(void *obj, int cpu)
+{
+	unsigned long hash;
+	struct overflow *ovfl;
+	struct list_head *pos, *n;
+	struct paygo *p;
+
+	hash = hash_function(obj);
+	p = per_cpu(paygo_table_ptr, cpu);
+
+// If the entry we are looking for does not exist in the overflow list, we should try again
+// because the owner of the hash table may have moved the entry from that overflow list to the hashtable.
+retry:
+	if (p->entries[hash].obj == obj) {
+		atomic_dec(&p->entries[hash].anchor_counter);
+	} else {
+		ovfl = &p->overflow_lists[hash];
+		spin_lock(&ovfl->lock);
+		list_for_each_safe(pos, n, &ovfl->head) {
+			struct paygo_entry *ovfl_entry =
+				list_entry(pos, struct paygo_entry, list);
+			if (likely(ovfl_entry->obj == obj)) {
+				atomic_dec(&ovfl_entry->anchor_counter);
+				spin_unlock(&ovfl->lock);
+				return;
+			}
+		}
+		spin_unlock(&ovfl->lock);
+		goto retry;
+	}
+}
+
+void init_paygo_table(void)
+{
+	int cpu;
+	struct paygo *p;
+
+	// initialize all of cpu's hashtable
+	for_each_possible_cpu(cpu) {
+		p = kzalloc(sizeof(struct paygo), GFP_KERNEL);
+		if (!p) {
+			pr_err("Failed to allocate paygo table for CPU %d\n",
+			       cpu);
+			continue;
+		}
+
+		per_cpu(paygo_table_ptr, cpu) = p;
+
+		for (int j = 0; j < TABLESIZE; j++) {
+			p->entries[j].obj = NULL;
+			p->entries[j].local_counter = 0;
+			atomic_set(&p->entries[j].anchor_counter, 0);
+			spin_lock_init(&p->overflow_lists[j].lock);
+			INIT_LIST_HEAD(&p->overflow_lists[j].head);
+		}
+	}
+}
+EXPORT_SYMBOL(init_paygo_table);
+
+int paygo_inc(void *obj)
+{
+	int ret;
+	int cpu;
+	struct paygo_entry *entry;
+	cpu = get_cpu();
+
+	entry = find_hash(obj);
+	// if there is an entry!
+	if (entry) {
+		entry->local_counter += 1;
+		entry->x += 1;
+		record_anchor(cpu, obj);
+		ret = 0;
+		put_cpu();
+		return ret;
+	}
+
+	// if there isn't
+	ret = push_hash(obj);
+	record_anchor(cpu, obj);
+	put_cpu();
+	return ret;
+}
+EXPORT_SYMBOL(paygo_inc);
+
+int paygo_dec(void *obj)
+{
+	int cpu;
+	int anchor_cpu;
+	struct paygo_entry *entry;
+	cpu = get_cpu();
+
+	anchor_cpu = unrecord_anchor(obj);
+
+	// local operation
+	if (likely(cpu == anchor_cpu)) {
+		entry = find_hash(obj);
+		// All unref operations are called after the ref operation is called.
+		// Therefore, there should never be a situation where there is no entry when doing an unref.
+		if (!entry) {
+			pr_info("paygo_dec: NULL return ERR!!!\n");
+			put_cpu();
+			return 0;
+		}
+		entry->local_counter -= 1;
+		entry->y -= 1;
+	}
+	// global operation
+	else {
+		dec_other_entry(obj, anchor_cpu);
+	}
+	put_cpu();
+	return 0;
+}
+EXPORT_SYMBOL(paygo_dec);
+
+bool paygo_read(void *obj)
+{
+	int mycpu;
+	int cur_cpu;
+	unsigned long hash;
+	struct paygo *p;
+	struct overflow *ovfl;
+	struct list_head *pos, *n;
+	struct paygo_entry *entry;
+
+	mycpu = get_cpu();
+	hash = hash_function(obj);
+
+	for_each_possible_cpu(cur_cpu) {
+		p = per_cpu(paygo_table_ptr, cur_cpu);
+		// self-checking
+		if (unlikely(mycpu == cur_cpu)) {
+			entry = find_hash(obj);
+			if (entry) {
+				// There can be a dec_other operation.
+				// Therefore, we need to check total count at this time.
+				if (entry->local_counter +
+					    atomic_read(
+						    &(entry->anchor_counter)) >
+				    0) {
+					put_cpu();
+					return false;
+				}
+			}
+			continue;
+		}
+		// check other cpu
+		else {
+			// Unlike dec_other, in paygo_read there is no 100% certainty
+			// that there is an entry in the hashtable (or in the overflow list).
+			// Therefore, we must first prevent the hashtable's owner from removing a entry from the overflow list
+			// and adding it to the hashtable. (this step is done in find_hash)
+			ovfl = &p->overflow_lists[hash];
+			spin_lock(&ovfl->lock);
+
+			entry = &p->entries[hash];
+			if (entry->obj == obj) {
+				if (entry->local_counter +
+					    atomic_read(
+						    &(entry->anchor_counter)) >
+				    0) {
+					spin_unlock(&ovfl->lock);
+					put_cpu();
+					return false;
+				}
+			} else {
+				list_for_each_safe(pos, n, &ovfl->head) {
+					struct paygo_entry *ovfl_entry =
+						list_entry(pos,
+							   struct paygo_entry,
+							   list);
+					if (ovfl_entry->obj == obj) {
+						if (ovfl_entry->local_counter +
+							    atomic_read(&(
+								    ovfl_entry
+									    ->anchor_counter)) >
+						    0) {
+							spin_unlock(
+								&ovfl->lock);
+							put_cpu();
+							return false;
+						}
+					}
+				}
+			}
+			spin_unlock(&ovfl->lock);
+			continue;
+		}
+	}
+
+	put_cpu();
+	return true;
+}
+EXPORT_SYMBOL(paygo_read);
+
+void traverse_paygo(void)
+{
+	struct paygo *p;
+	struct paygo_entry *entry;
+	struct list_head *cur;
+	int i;
+	int cpu;
+	int ovfl_length;
+
+	entry = NULL;
+
+	for_each_possible_cpu(cpu) {
+		p = per_cpu(paygo_table_ptr, cpu);
+		printk(KERN_INFO "CPU %d:\n", cpu);
+
+		for (i = 0; i < TABLESIZE; i++) {
+			ovfl_length = 1;
+			entry = &p->entries[i];
+			if (entry->obj) {
+				printk(KERN_INFO
+				       "  Entry %d: obj=%p, local_counter=%d, anchor_counter=%d total_count=%d (x=%d y=%d)\n",
+				       i, entry->obj, entry->local_counter,
+				       atomic_read(&entry->anchor_counter),
+				       entry->local_counter +
+					       atomic_read(
+						       &entry->anchor_counter),
+				       entry->x, entry->y);
+			} else {
+				printk(KERN_INFO
+				       "  Entry %d: obj=%p, local_counter=%d, anchor_counter=%d total_count=%d (x=%d y=%d)\n",
+				       i, entry->obj, entry->local_counter,
+				       atomic_read(&entry->anchor_counter),
+				       entry->local_counter +
+					       atomic_read(
+						       &entry->anchor_counter),
+				       entry->x, entry->y);
+			}
+			list_for_each(cur, &p->overflow_lists[i].head) {
+				entry = list_entry(cur, struct paygo_entry,
+						   list);
+				printk(KERN_INFO
+				       "  \tOverflow Entry %d-%d: obj=%p, local_counter=%d, anchor_counter=%d total_count=%d (x=%d y=%d)\n",
+				       i, ovfl_length, entry->obj,
+				       entry->local_counter,
+				       atomic_read(&entry->anchor_counter),
+				       entry->local_counter +
+					       atomic_read(
+						       &entry->anchor_counter),
+				       entry->x, entry->y);
+				ovfl_length++;
+			}
+		}
+	}
+}
+EXPORT_SYMBOL(traverse_paygo);
