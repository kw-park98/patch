diff --git a/block/partitions/core.c b/block/partitions/core.c
index b8112f52d..d44a46006 100644
--- a/block/partitions/core.c
+++ b/block/partitions/core.c
@@ -717,7 +717,7 @@ void *read_part_sector(struct parsed_partitions *state, sector_t n, Sector *p)
 	folio = read_mapping_folio(mapping, n >> PAGE_SECTORS_SHIFT, NULL);
 	if (IS_ERR(folio))
 		goto out;
-
+	
 	p->v = folio;
 	return folio_address(folio) + offset_in_folio(folio, n * SECTOR_SIZE);
 out:
diff --git a/block/partitions/efi.c b/block/partitions/efi.c
index 5e9be13a5..5d719ad84 100644
--- a/block/partitions/efi.c
+++ b/block/partitions/efi.c
@@ -87,6 +87,7 @@
 #include <linux/ctype.h>
 #include <linux/math64.h>
 #include <linux/slab.h>
+#include <linux/myref.h>
 #include "check.h"
 #include "efi.h"
 
@@ -251,7 +252,8 @@ static size_t read_lba(struct parsed_partitions *state,
 		if (copied > count)
 			copied = count;
 		memcpy(buffer, data, copied);
-		put_dev_sector(sect);
+		//put_dev_sector(sect);
+		my_folio_put(sect.v);
 		buffer += copied;
 		totalreadcount +=copied;
 		count -= copied;
diff --git a/include/linux/myref.h b/include/linux/myref.h
new file mode 100644
index 000000000..b58467d64
--- /dev/null
+++ b/include/linux/myref.h
@@ -0,0 +1,12 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _LINUX_MYREF_H
+#define _LINUX_MYREF_H
+
+#include <linux/fs.h>
+#include <linux/mm_types.h>
+
+int my_folio_get(struct folio *folio);
+int my_folio_put(struct folio *folio);
+bool my_folio_can_freeze(struct folio *folio, int count);
+
+#endif /* _LINUX_MYREF_H */
diff --git a/include/linux/pagemap.h b/include/linux/pagemap.h
index 29e1f9e76..5b6409c62 100644
--- a/include/linux/pagemap.h
+++ b/include/linux/pagemap.h
@@ -1280,7 +1280,6 @@ static inline struct page *readahead_page(struct readahead_control *ractl)
 static inline struct folio *readahead_folio(struct readahead_control *ractl)
 {
 	struct folio *folio = __readahead_folio(ractl);
-
 	if (folio)
 		folio_put(folio);
 	return folio;
diff --git a/include/linux/paygo.h b/include/linux/paygo.h
new file mode 100644
index 000000000..3f1491b4b
--- /dev/null
+++ b/include/linux/paygo.h
@@ -0,0 +1,48 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _LINUX_PAYGO_H
+#define _LINUX_PAYGO_H
+
+
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <asm/atomic.h>
+
+
+#define TABLESIZE (4)
+#define HASHSHIFT (11)
+
+struct paygo_entry {
+	void *obj;
+	int local_counter;
+	atomic_t anchor_counter;
+	struct list_head list;
+
+
+	// x,y are used for debug
+	// x means local inc to the obj
+	// y means local dec to the obj
+
+	// (x + y) = entry->local_counter (always)
+	// (x + y) + anchor_counter = 0 when the program end (i.e. After all inc and dec pairs have been terminated)
+	int x;
+	int y;
+} ____cacheline_aligned_in_smp;
+
+struct overflow {
+	spinlock_t lock;
+	struct list_head head;
+};
+
+struct paygo {
+	struct paygo_entry entries[TABLESIZE];
+	struct overflow overflow_lists[TABLESIZE];
+};
+
+void init_paygo_table(void);
+int paygo_inc(void *obj);
+int paygo_dec(void *obj);
+bool paygo_read(void *obj);
+void traverse_paygo(void);
+
+
+#endif /* _LINUX_PAYGO_H */
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 853d08f75..03b6ada97 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -734,6 +734,12 @@ struct kmap_ctrl {
 #endif
 };
 
+struct anchor_info {
+	int cpu;
+	void *obj;
+	struct list_head list;
+};
+
 struct task_struct {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/*
@@ -749,6 +755,8 @@ struct task_struct {
 	unsigned int			saved_state;
 #endif
 
+	struct list_head anchor_info_list;
+
 	/*
 	 * This begins the randomizable portion of task_struct. Only
 	 * scheduling-critical items should be added above here.
diff --git a/init/init_task.c b/init/init_task.c
index ff6c4b9bf..0eb4ea518 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -71,6 +71,7 @@ struct task_struct init_task
 	.thread_info	= INIT_THREAD_INFO(init_task),
 	.stack_refcount	= REFCOUNT_INIT(1),
 #endif
+	.anchor_info_list = LIST_HEAD_INIT(init_task.anchor_info_list),
 	.__state	= 0,
 	.stack		= init_stack,
 	.usage		= REFCOUNT_INIT(2),
diff --git a/kernel/fork.c b/kernel/fork.c
index 9f7fe3541..8b555b78b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2147,6 +2147,9 @@ static __latent_entropy struct task_struct *copy_process(
 	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
 	p->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE | PF_NO_SETAFFINITY);
 	p->flags |= PF_FORKNOEXEC;
+	
+	INIT_LIST_HEAD(&p->anchor_info_list);
+
 	INIT_LIST_HEAD(&p->children);
 	INIT_LIST_HEAD(&p->sibling);
 	rcu_copy_process(p);
diff --git a/mm/Makefile b/mm/Makefile
index 8e105e5b3..132145282 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -66,6 +66,8 @@ memory-hotplug-$(CONFIG_MEMORY_HOTPLUG) += memory_hotplug.o
 obj-y += page-alloc.o
 obj-y += init-mm.o
 obj-y += memblock.o
+obj-y += paygo.o
+obj-y += myref.o
 obj-y += $(memory-hotplug-y)
 
 ifdef CONFIG_MMU
diff --git a/mm/filemap.c b/mm/filemap.c
index 0e20a8d6d..df6d03885 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -10,6 +10,8 @@
  * most "normal" filesystems (but you don't /have/ to use this:
  * the NFS filesystem used to do this differently, for example)
  */
+#include <linux/paygo.h>
+#include <linux/myref.h>
 #include <linux/export.h>
 #include <linux/compiler.h>
 #include <linux/dax.h>
@@ -225,6 +227,7 @@ void __filemap_remove_folio(struct folio *folio, void *shadow)
 
 void filemap_free_folio(struct address_space *mapping, struct folio *folio)
 {
+	int i;
 	void (*free_folio)(struct folio *);
 	int refs = 1;
 
@@ -234,7 +237,10 @@ void filemap_free_folio(struct address_space *mapping, struct folio *folio)
 
 	if (folio_test_large(folio) && !folio_test_hugetlb(folio))
 		refs = folio_nr_pages(folio);
-	folio_put_refs(folio, refs);
+	//folio_put_refs(folio, refs);
+	for(i=0; i<refs; i++) {
+		my_folio_put(folio);
+	}
 }
 
 /**
@@ -812,6 +818,7 @@ void replace_page_cache_folio(struct folio *old, struct folio *new)
 	VM_BUG_ON_FOLIO(new->mapping, new);
 
 	folio_get(new);
+	//my_folio_get(new);
 	new->mapping = mapping;
 	new->index = offset;
 
@@ -834,12 +841,14 @@ void replace_page_cache_folio(struct folio *old, struct folio *new)
 	if (free_folio)
 		free_folio(old);
 	folio_put(old);
+	//my_folio_put(old);
 }
 EXPORT_SYMBOL_GPL(replace_page_cache_folio);
 
 noinline int __filemap_add_folio(struct address_space *mapping,
 		struct folio *folio, pgoff_t index, gfp_t gfp, void **shadowp)
 {
+	int i;
 	XA_STATE(xas, &mapping->i_pages, index);
 	int huge = folio_test_hugetlb(folio);
 	bool charged = false;
@@ -860,7 +869,10 @@ noinline int __filemap_add_folio(struct address_space *mapping,
 	}
 
 	gfp &= GFP_RECLAIM_MASK;
-	folio_ref_add(folio, nr);
+	//folio_ref_add(folio, nr);
+	for(i = 0; i < nr; i++) {
+		my_folio_get(folio);
+	}
 	folio->mapping = mapping;
 	folio->index = xas.xa_index;
 
@@ -920,7 +932,10 @@ noinline int __filemap_add_folio(struct address_space *mapping,
 		mem_cgroup_uncharge(folio);
 	folio->mapping = NULL;
 	/* Leave page->index set: truncation relies upon it */
-	folio_put_refs(folio, nr);
+	//folio_put_refs(folio, nr);
+	for(i = 0; i < nr; i++) {
+		my_folio_put(folio);
+	}
 	return xas_error(&xas);
 }
 ALLOW_ERROR_INJECTION(__filemap_add_folio, ERRNO);
@@ -1039,6 +1054,7 @@ void __init pagecache_init(void)
 		init_waitqueue_head(&folio_wait_table[i]);
 
 	page_writeback_init();
+	init_paygo_table();
 }
 
 /*
@@ -1275,7 +1291,8 @@ static inline int folio_wait_bit_common(struct folio *folio, int bit_nr,
 	 * We can drop our reference to the folio.
 	 */
 	if (behavior == DROP)
-		folio_put(folio);
+		//folio_put(folio);
+		my_folio_put(folio);
 
 	/*
 	 * Note that until the "finish_wait()", or until
@@ -1545,6 +1562,7 @@ void folio_end_private_2(struct folio *folio)
 	clear_bit_unlock(PG_private_2, folio_flags(folio, 0));
 	folio_wake_bit(folio, PG_private_2);
 	folio_put(folio);
+	//my_folio_put(folio);
 }
 EXPORT_SYMBOL(folio_end_private_2);
 
@@ -1611,6 +1629,7 @@ void folio_end_writeback(struct folio *folio)
 	 * reused before the folio_wake().
 	 */
 	folio_get(folio);
+	//my_folio_get(folio);
 	if (!__folio_end_writeback(folio))
 		BUG();
 
@@ -1618,6 +1637,7 @@ void folio_end_writeback(struct folio *folio)
 	folio_wake(folio, PG_writeback);
 	acct_reclaim_writeback(folio);
 	folio_put(folio);
+	//my_folio_put(folio);
 }
 EXPORT_SYMBOL(folio_end_writeback);
 
@@ -1861,11 +1881,13 @@ static void *mapping_get_entry(struct address_space *mapping, pgoff_t index)
 	if (!folio || xa_is_value(folio))
 		goto out;
 
-	if (!folio_try_get_rcu(folio))
+	//if (!folio_try_get_rcu(folio))
+	if(!my_folio_get(folio))
 		goto repeat;
 
 	if (unlikely(folio != xas_reload(&xas))) {
-		folio_put(folio);
+		//folio_put(folio);
+		my_folio_put(folio);
 		goto repeat;
 	}
 out:
@@ -1925,7 +1947,8 @@ struct folio *__filemap_get_folio(struct address_space *mapping, pgoff_t index,
 	if (fgp_flags & FGP_LOCK) {
 		if (fgp_flags & FGP_NOWAIT) {
 			if (!folio_trylock(folio)) {
-				folio_put(folio);
+				//folio_put(folio);
+				my_folio_put(folio);
 				return NULL;
 			}
 		} else {
@@ -1935,7 +1958,8 @@ struct folio *__filemap_get_folio(struct address_space *mapping, pgoff_t index,
 		/* Has the page been truncated? */
 		if (unlikely(folio->mapping != mapping)) {
 			folio_unlock(folio);
-			folio_put(folio);
+			//folio_put(folio);
+			my_folio_put(folio);
 			goto repeat;
 		}
 		VM_BUG_ON_FOLIO(!folio_contains(folio, index), folio);
@@ -1976,6 +2000,7 @@ struct folio *__filemap_get_folio(struct address_space *mapping, pgoff_t index,
 
 		err = filemap_add_folio(mapping, folio, index, gfp);
 		if (unlikely(err)) {
+			// this folio_put is used to free the folio allocated. (folio_put makes free the folio when ref==0.
 			folio_put(folio);
 			folio = NULL;
 			if (err == -EEXIST)
@@ -2016,10 +2041,12 @@ static inline struct folio *find_get_entry(struct xa_state *xas, pgoff_t max,
 		return folio;
 
 	if (!folio_try_get_rcu(folio))
+	//if (!my_folio_get(folio))
 		goto reset;
 
 	if (unlikely(folio != xas_reload(xas))) {
 		folio_put(folio);
+		//my_folio_put(folio);
 		goto reset;
 	}
 
@@ -2124,6 +2151,7 @@ unsigned find_lock_entries(struct address_space *mapping, pgoff_t *start,
 		folio_unlock(folio);
 put:
 		folio_put(folio);
+		//my_folio_put(folio);
 	}
 	rcu_read_unlock();
 
@@ -2244,6 +2272,7 @@ unsigned filemap_get_folios_contig(struct address_space *mapping,
 			goto update_start;
 
 		if (!folio_try_get_rcu(folio))
+		//if(!my_folio_get(folio))
 			goto retry;
 
 		if (unlikely(folio != xas_reload(&xas)))
@@ -2260,6 +2289,7 @@ unsigned filemap_get_folios_contig(struct address_space *mapping,
 		continue;
 put_folio:
 		folio_put(folio);
+		//my_folio_put(folio);
 
 retry:
 		xas_reset(&xas);
@@ -2384,8 +2414,9 @@ static void filemap_get_read_batch(struct address_space *mapping,
 			break;
 		if (xa_is_sibling(folio))
 			break;
-		if (!folio_try_get_rcu(folio))
-			goto retry;
+		//if (!folio_try_get_rcu(folio))
+		if(!my_folio_get(folio))
+			goto retry;	
 
 		if (unlikely(folio != xas_reload(&xas)))
 			goto put_folio;
@@ -2399,7 +2430,8 @@ static void filemap_get_read_batch(struct address_space *mapping,
 		xas_advance(&xas, folio->index + folio_nr_pages(folio) - 1);
 		continue;
 put_folio:
-		folio_put(folio);
+		//folio_put(folio);
+		my_folio_put(folio);
 retry:
 		xas_reset(&xas);
 	}
@@ -2756,7 +2788,8 @@ ssize_t filemap_read(struct kiocb *iocb, struct iov_iter *iter,
 		}
 put_folios:
 		for (i = 0; i < folio_batch_count(&fbatch); i++)
-			folio_put(fbatch.folios[i]);
+			//folio_put(fbatch.folios[i]);
+			my_folio_put(fbatch.folios[i]);
 		folio_batch_init(&fbatch);
 	} while (iov_iter_count(iter) && iocb->ki_pos < isize && !error);
 
@@ -2934,6 +2967,7 @@ loff_t mapping_seek_hole_data(struct address_space *mapping, loff_t start,
 			xas_set(&xas, pos >> PAGE_SHIFT);
 		if (!xa_is_value(folio))
 			folio_put(folio);
+			//my_folio_put(folio);
 	}
 	if (seek_data)
 		start = -ENXIO;
@@ -2941,6 +2975,7 @@ loff_t mapping_seek_hole_data(struct address_space *mapping, loff_t start,
 	rcu_read_unlock();
 	if (folio && !xa_is_value(folio))
 		folio_put(folio);
+		//my_folio_put(folio);
 	if (start > end)
 		return end;
 	return start;
@@ -3180,6 +3215,7 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 	if (unlikely(folio->mapping != mapping)) {
 		folio_unlock(folio);
 		folio_put(folio);
+		//my_folio_put(folio);
 		goto retry_find;
 	}
 	VM_BUG_ON_FOLIO(!folio_contains(folio, index), folio);
@@ -3198,6 +3234,7 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 		if (!mapping_locked) {
 			folio_unlock(folio);
 			folio_put(folio);
+			//my_folio_put(folio);
 			goto retry_find;
 		}
 		goto page_not_uptodate;
@@ -3223,6 +3260,7 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 	if (unlikely(index >= max_idx)) {
 		folio_unlock(folio);
 		folio_put(folio);
+		//my_folio_put(folio);
 		return VM_FAULT_SIGBUS;
 	}
 
@@ -3241,6 +3279,7 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 	if (fpin)
 		goto out_retry;
 	folio_put(folio);
+	//my_folio_put(folio);
 
 	if (!error || error == AOP_TRUNCATED_PAGE)
 		goto retry_find;
@@ -3256,6 +3295,7 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 	 */
 	if (folio)
 		folio_put(folio);
+		//my_folio_put(folio);
 	if (mapping_locked)
 		filemap_invalidate_unlock_shared(mapping);
 	if (fpin)
@@ -3313,7 +3353,9 @@ static struct folio *next_uptodate_page(struct folio *folio,
 		if (folio_test_locked(folio))
 			continue;
 		if (!folio_try_get_rcu(folio))
+		//if(!my_folio_get(folio))
 			continue;
+
 		/* Has the page moved or been split? */
 		if (unlikely(folio != xas_reload(xas)))
 			goto skip;
@@ -3333,6 +3375,7 @@ static struct folio *next_uptodate_page(struct folio *folio,
 		folio_unlock(folio);
 skip:
 		folio_put(folio);
+		//my_folio_put(folio);
 	} while ((folio = xas_next_entry(xas, end_pgoff)) != NULL);
 
 	return NULL;
@@ -3422,6 +3465,7 @@ vm_fault_t filemap_map_pages(struct vm_fault *vmf,
 		}
 		folio_unlock(folio);
 		folio_put(folio);
+		//my_folio_put(folio);
 	} while ((folio = next_map_page(mapping, &xas, end_pgoff)) != NULL);
 	pte_unmap_unlock(vmf->pte, vmf->ptl);
 out:
@@ -3520,6 +3564,8 @@ static struct folio *do_read_cache_folio(struct address_space *mapping,
 			return ERR_PTR(-ENOMEM);
 		err = filemap_add_folio(mapping, folio, index, gfp);
 		if (unlikely(err)) {
+			// this folio_put => make reference count 1->0 to free the err page.
+			// so do not use my_folio_put
 			folio_put(folio);
 			if (err == -EEXIST)
 				goto repeat;
@@ -3540,7 +3586,8 @@ static struct folio *do_read_cache_folio(struct address_space *mapping,
 	/* Folio was truncated from mapping */
 	if (!folio->mapping) {
 		folio_unlock(folio);
-		folio_put(folio);
+		//folio_put(folio);
+		my_folio_put(folio);
 		goto repeat;
 	}
 
@@ -3554,6 +3601,7 @@ static struct folio *do_read_cache_folio(struct address_space *mapping,
 	err = filemap_read_folio(file, filler, folio);
 	if (err) {
 		folio_put(folio);
+		//my_folio_put(folio);
 		if (err == AOP_TRUNCATED_PAGE)
 			goto repeat;
 		return ERR_PTR(err);
diff --git a/mm/myref.c b/mm/myref.c
new file mode 100644
index 000000000..3f8c774a3
--- /dev/null
+++ b/mm/myref.c
@@ -0,0 +1,47 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * New page cache protocol
+ *
+ * Copyright (C) 2023 Hanyang, Univ. Kun-wook Park
+ */
+
+#include <linux/paygo.h>
+#include <linux/fs.h>
+#include <linux/mm_types.h>
+#include <linux/page_ref.h>
+
+int my_folio_get(struct folio *folio)
+{
+	int refcount;
+	paygo_inc((void*)&folio->page);	
+	refcount = folio_ref_count(folio);	
+	if(refcount) {
+		return true;
+	}
+	paygo_dec((void*)&folio->page);	
+	return false;
+}
+EXPORT_SYMBOL(my_folio_get);
+
+int my_folio_put(struct folio *folio)
+{
+	paygo_dec((void*)&folio->page);	
+	return 0;
+}
+EXPORT_SYMBOL(my_folio_put);
+
+bool my_folio_can_freeze(struct folio *folio, int count)
+{
+	bool can_freeze;
+	if(!folio_ref_freeze(folio, count)) {
+		// CAS fail: It means that another process already starts delete the folio.
+		return false;
+	}	
+	// ref = 0 in here
+	can_freeze = paygo_read((void*)&folio->page);
+	if(can_freeze) 
+		return true;
+	// can_freeze(false) : There may be the users who use the folio.
+	return false;
+}
+EXPORT_SYMBOL(my_folio_can_freeze);
diff --git a/mm/paygo.c b/mm/paygo.c
new file mode 100644
index 000000000..c8eec9490
--- /dev/null
+++ b/mm/paygo.c
@@ -0,0 +1,436 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Paygo reference counting
+ *
+ * Copyright (C) 2023 Hanyang, Univ. Kun-wook Park
+ */
+
+#include <linux/paygo.h>
+#include <linux/percpu.h>
+#include <linux/list.h>
+#include <linux/hashtable.h>
+#include <linux/spinlock.h>
+#include <linux/percpu.h>
+#include <linux/slab.h>
+#include <asm/atomic.h>
+#include <linux/hash.h>
+#include <linux/sched.h>
+
+static DEFINE_PER_CPU(struct paygo *, paygo_table_ptr);
+
+static unsigned long hash_function(const void *obj)
+{
+	unsigned long ret;
+	unsigned long hash;
+	hash = hash_64((unsigned long)obj, HASHSHIFT);
+	ret = hash % TABLESIZE;
+	return ret;
+}
+
+static int push_hash(void *obj)
+{
+	unsigned long hash;
+	struct paygo_entry *entry;
+	struct overflow *ovfl;
+	struct paygo *p = per_cpu(paygo_table_ptr, smp_processor_id());
+
+	hash = hash_function(obj);
+	entry = &p->entries[hash];
+
+	// when the hashtable's entry is NULL
+	if (entry->obj == NULL) {
+		{
+			entry->obj = obj;
+			entry->local_counter = 1;
+			atomic_set(&entry->anchor_counter, 0);
+
+			entry->x = 1;
+			entry->y = 0;
+		}
+		return 0;
+	}
+
+	// when there is NULL in the entry
+	// We need to insert a new entry into the overflow list
+	else {
+		struct paygo_entry *new_entry;
+		ovfl = &p->overflow_lists[hash];
+		new_entry = kzalloc(sizeof(struct paygo_entry), GFP_ATOMIC);
+		if (!new_entry) {
+			return -ENOMEM;
+		}
+		{
+			new_entry->obj = obj;
+			new_entry->local_counter = 1;
+			atomic_set(&new_entry->anchor_counter, 0);
+
+			new_entry->x = 1;
+			new_entry->y = 0;
+		}
+
+		spin_lock(&ovfl->lock);
+		list_add(&new_entry->list, &ovfl->head);
+		spin_unlock(&ovfl->lock);
+		return 0;
+	}
+}
+
+static struct paygo_entry *find_hash(void *obj)
+{
+	int cpu;
+	unsigned long hash;
+	struct paygo_entry *entry;
+	struct overflow *ovfl;
+	struct list_head *pos, *n;
+	struct paygo *p;
+	cpu = smp_processor_id();
+	p = per_cpu(paygo_table_ptr, cpu);
+
+	hash = hash_function(obj);
+
+	entry = &p->entries[hash];
+
+// redo to delete a new entry when the sum of the new entry's counter taken from the overflow list is 0
+redo:
+	if (likely(entry->obj == obj)) {
+		return entry;
+	} else {
+		ovfl = &p->overflow_lists[hash];
+		spin_lock(&ovfl->lock);
+		///////////////////////////////////////////////////////////////////////////////////////////
+		// delete entry on hash table
+		if (unlikely(entry->local_counter +
+				     atomic_read(&(entry->anchor_counter)) ==
+			     0)) {
+			struct paygo_entry *new_entry;
+			if (!list_empty(&ovfl->head)) {
+				new_entry = list_first_entry(
+					&ovfl->head, struct paygo_entry, list);
+				*entry = *new_entry;
+				list_del(&new_entry->list);
+				kfree(new_entry);
+				spin_unlock(&ovfl->lock);
+				// we need to redo and check
+				// 0. Whether or not a new entry in the hashtable is what we were looking for
+				// 1. Whether or not the overflow list is empty
+				goto redo;
+			}
+
+			else {
+				entry->obj = NULL;
+				entry->local_counter = 0;
+				atomic_set(&(entry->anchor_counter), 0);
+				entry->x = 0;
+				entry->y = 0;
+
+				spin_unlock(&ovfl->lock);
+				return NULL;
+			}
+		}
+		///////////////////////////////////////////////////////////////////////////////////////////
+
+		// the hashtable's entry is not the entry that we are looking for.
+		// so we need to search overflow list
+		list_for_each_safe(pos, n, &ovfl->head) {
+			struct paygo_entry *ovfl_entry =
+				list_entry(pos, struct paygo_entry, list);
+
+			if (ovfl_entry->obj == obj) {
+				spin_unlock(&ovfl->lock);
+				return ovfl_entry;
+			}
+		}
+		spin_unlock(&ovfl->lock);
+	}
+
+	return NULL;
+}
+
+static void record_anchor(int cpu, void *obj)
+{
+	struct anchor_info *info;
+	info = kmalloc(sizeof(struct anchor_info), GFP_KERNEL);
+	if (!info) {
+		pr_err("Failed to allocate memory for anchor_info\n");
+		return;
+	}
+	info->cpu = cpu;
+	info->obj = obj;
+	list_add_tail(&info->list, &current->anchor_info_list);
+}
+
+static int unrecord_anchor(void *obj)
+{
+	struct anchor_info *info;
+	int cpu = -1;
+
+	list_for_each_entry_reverse(
+		info, &current->anchor_info_list, list) {
+		if (info->obj == obj) {
+			cpu = info->cpu;
+			list_del(&info->list);
+			kfree(info);
+			break;
+		}
+	}
+
+	// Since all of the unref operations are always followed by ref operations,
+	// there is no situation where anchor information list is empty.
+	if (cpu == -1) {
+		pr_err("Failed to find anchor_info with given obj\n");
+	}
+
+	return cpu;
+}
+
+static void dec_other_entry(void *obj, int cpu)
+{
+	unsigned long hash;
+	struct overflow *ovfl;
+	struct list_head *pos, *n;
+	struct paygo *p;
+
+	hash = hash_function(obj);
+	p = per_cpu(paygo_table_ptr, cpu);
+
+// If the entry we are looking for does not exist in the overflow list, we should try again
+// because the owner of the hash table may have moved the entry from that overflow list to the hashtable.
+retry:
+	if (p->entries[hash].obj == obj) {
+		atomic_dec(&p->entries[hash].anchor_counter);
+	} else {
+		ovfl = &p->overflow_lists[hash];
+		spin_lock(&ovfl->lock);
+		list_for_each_safe(pos, n, &ovfl->head) {
+			struct paygo_entry *ovfl_entry =
+				list_entry(pos, struct paygo_entry, list);
+			if (likely(ovfl_entry->obj == obj)) {
+				atomic_dec(&ovfl_entry->anchor_counter);
+				spin_unlock(&ovfl->lock);
+				return;
+			}
+		}
+		spin_unlock(&ovfl->lock);
+		goto retry;
+	}
+}
+
+void init_paygo_table(void)
+{
+	int cpu;
+	struct paygo *p;
+
+	// initialize all of cpu's hashtable
+	for_each_possible_cpu(cpu) {
+		p = kzalloc(sizeof(struct paygo), GFP_KERNEL);
+		if (!p) {
+			pr_err("Failed to allocate paygo table for CPU %d\n",
+			       cpu);
+			continue;
+		}
+
+		per_cpu(paygo_table_ptr, cpu) = p;
+
+		for (int j = 0; j < TABLESIZE; j++) {
+			p->entries[j].obj = NULL;
+			p->entries[j].local_counter = 0;
+			atomic_set(&p->entries[j].anchor_counter, 0);
+			spin_lock_init(&p->overflow_lists[j].lock);
+			INIT_LIST_HEAD(&p->overflow_lists[j].head);
+		}
+	}
+}
+EXPORT_SYMBOL(init_paygo_table);
+
+int paygo_inc(void *obj)
+{
+	int ret;
+	int cpu;
+	struct paygo_entry *entry;
+	cpu = get_cpu();
+
+	entry = find_hash(obj);
+	// if there is an entry!
+	if (entry) {
+		entry->local_counter += 1;
+		entry->x += 1;
+		record_anchor(cpu, obj);
+		ret = 0;
+		put_cpu();
+		return ret;
+	}
+
+	// if there isn't
+	ret = push_hash(obj);
+	record_anchor(cpu, obj);
+	put_cpu();
+	return ret;
+}
+EXPORT_SYMBOL(paygo_inc);
+
+int paygo_dec(void *obj)
+{
+	int cpu;
+	int anchor_cpu;
+	struct paygo_entry *entry;
+	cpu = get_cpu();
+
+	anchor_cpu = unrecord_anchor(obj);
+
+	// local operation
+	if (likely(cpu == anchor_cpu)) {
+		entry = find_hash(obj);
+		// All unref operations are called after the ref operation is called.
+		// Therefore, there should never be a situation where there is no entry when doing an unref.
+		if (!entry) {
+			pr_info("paygo_dec: NULL return ERR!!!\n");
+			put_cpu();
+			return 0;
+		}
+		entry->local_counter -= 1;
+		entry->y -= 1;
+	}
+	// global operation
+	else {
+		dec_other_entry(obj, anchor_cpu);
+	}
+	put_cpu();
+	return 0;
+}
+EXPORT_SYMBOL(paygo_dec);
+
+bool paygo_read(void *obj)
+{
+	int mycpu;
+	int cur_cpu;
+	unsigned long hash;
+	struct paygo *p;
+	struct overflow *ovfl;
+	struct list_head *pos, *n;
+	struct paygo_entry *entry;
+
+	mycpu = get_cpu();
+	hash = hash_function(obj);
+
+	for_each_possible_cpu(cur_cpu) {
+		p = per_cpu(paygo_table_ptr, cur_cpu);
+		// self-checking
+		if (unlikely(mycpu == cur_cpu)) {
+			entry = find_hash(obj);
+			if (entry) {
+				// There can be a dec_other operation.
+				// Therefore, we need to check total count at this time.
+				if (entry->local_counter +
+					    atomic_read(
+						    &(entry->anchor_counter)) >
+				    0) {
+					put_cpu();
+					return false;
+				}
+			}
+			continue;
+		}
+		// check other cpu
+		else {
+			// Unlike dec_other, in paygo_read there is no 100% certainty
+			// that there is an entry in the hashtable (or in the overflow list).
+			// Therefore, we must first prevent the hashtable's owner from removing a entry from the overflow list
+			// and adding it to the hashtable. (this step is done in find_hash)
+			ovfl = &p->overflow_lists[hash];
+			spin_lock(&ovfl->lock);
+
+			entry = &p->entries[hash];
+			if (entry->obj == obj) {
+				if (entry->local_counter +
+					    atomic_read(
+						    &(entry->anchor_counter)) >
+				    0) {
+					spin_unlock(&ovfl->lock);
+					put_cpu();
+					return false;
+				}
+			} else {
+				list_for_each_safe(pos, n, &ovfl->head) {
+					struct paygo_entry *ovfl_entry =
+						list_entry(pos,
+							   struct paygo_entry,
+							   list);
+					if (ovfl_entry->obj == obj) {
+						if (ovfl_entry->local_counter +
+							    atomic_read(&(
+								    ovfl_entry
+									    ->anchor_counter)) >
+						    0) {
+							spin_unlock(
+								&ovfl->lock);
+							put_cpu();
+							return false;
+						}
+					}
+				}
+			}
+			spin_unlock(&ovfl->lock);
+			continue;
+		}
+	}
+
+	put_cpu();
+	return true;
+}
+EXPORT_SYMBOL(paygo_read);
+
+void traverse_paygo(void)
+{
+	struct paygo *p;
+	struct paygo_entry *entry;
+	struct list_head *cur;
+	int i;
+	int cpu;
+	int ovfl_length;
+
+	entry = NULL;
+
+	for_each_possible_cpu(cpu) {
+		p = per_cpu(paygo_table_ptr, cpu);
+		printk(KERN_INFO "CPU %d:\n", cpu);
+
+		for (i = 0; i < TABLESIZE; i++) {
+			ovfl_length = 1;
+			entry = &p->entries[i];
+			if (entry->obj) {
+				printk(KERN_INFO
+				       "  Entry %d: obj=%p, local_counter=%d, anchor_counter=%d total_count=%d (x=%d y=%d)\n",
+				       i, entry->obj, entry->local_counter,
+				       atomic_read(&entry->anchor_counter),
+				       entry->local_counter +
+					       atomic_read(
+						       &entry->anchor_counter),
+				       entry->x, entry->y);
+			} else {
+				printk(KERN_INFO
+				       "  Entry %d: obj=%p, local_counter=%d, anchor_counter=%d total_count=%d (x=%d y=%d)\n",
+				       i, entry->obj, entry->local_counter,
+				       atomic_read(&entry->anchor_counter),
+				       entry->local_counter +
+					       atomic_read(
+						       &entry->anchor_counter),
+				       entry->x, entry->y);
+			}
+			list_for_each(cur, &p->overflow_lists[i].head) {
+				entry = list_entry(cur, struct paygo_entry,
+						   list);
+				printk(KERN_INFO
+				       "  \tOverflow Entry %d-%d: obj=%p, local_counter=%d, anchor_counter=%d total_count=%d (x=%d y=%d)\n",
+				       i, ovfl_length, entry->obj,
+				       entry->local_counter,
+				       atomic_read(&entry->anchor_counter),
+				       entry->local_counter +
+					       atomic_read(
+						       &entry->anchor_counter),
+				       entry->x, entry->y);
+				ovfl_length++;
+			}
+		}
+	}
+}
+EXPORT_SYMBOL(traverse_paygo);
diff --git a/mm/readahead.c b/mm/readahead.c
index b10f0cf81..c4ea44dd4 100644
--- a/mm/readahead.c
+++ b/mm/readahead.c
@@ -152,7 +152,6 @@ static void read_pages(struct readahead_control *rac)
 
 	if (!readahead_count(rac))
 		return;
-
 	if (unlikely(rac->_workingset))
 		psi_memstall_enter(&rac->_pflags);
 	blk_start_plug(&plug);
